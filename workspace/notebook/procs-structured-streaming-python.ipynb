{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âŠ• [structured-streaming-python.html - Databricks](https://docs.databricks.com/_static/notebooks/structured-streaming-python.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:00:41.187039Z",
     "start_time": "2019-01-27T18:00:41.181765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiaofeiwu/jcloud/assets/langs/workspace/spark/workspace/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:03:19.966786Z",
     "start_time": "2019-01-27T18:03:14.814938Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:04:16.298618Z",
     "start_time": "2019-01-27T18:04:14.062161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time: timestamp, action: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|               time|action|\n",
      "+-------------------+------+\n",
      "|2016-07-26 10:45:07|  Open|\n",
      "|2016-07-26 10:45:47|  Open|\n",
      "|2016-07-26 10:46:42|  Open|\n",
      "|2016-07-26 10:46:59|  Open|\n",
      "|2016-07-26 10:47:05|  Open|\n",
      "|2016-07-26 10:47:14|  Open|\n",
      "|2016-07-26 10:47:25|  Open|\n",
      "|2016-07-26 10:47:26|  Open|\n",
      "|2016-07-26 10:47:28|  Open|\n",
      "|2016-07-26 10:47:36|  Open|\n",
      "|2016-07-26 10:47:44|  Open|\n",
      "|2016-07-26 10:47:46|  Open|\n",
      "|2016-07-26 10:47:47|  Open|\n",
      "|2016-07-26 10:47:49|  Open|\n",
      "|2016-07-26 10:47:51|  Open|\n",
      "|2016-07-26 10:48:02|  Open|\n",
      "|2016-07-26 10:48:05|  Open|\n",
      "|2016-07-26 10:48:11|  Open|\n",
      "|2016-07-26 10:48:17|  Open|\n",
      "|2016-07-26 10:48:23|  Open|\n",
      "+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "inputPath = \"./events/\"\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "# Static DataFrame representing data in the JSON files\n",
    "staticInputDF = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(jsonSchema)\n",
    "    .json(inputPath)\n",
    ")\n",
    "display(staticInputDF)\n",
    "staticInputDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the number of \"open\" and \"close\" actions with one hour windows. To do this, we will group by the action column and 1 hour windows over the time column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:04:53.961018Z",
     "start_time": "2019-01-27T18:04:53.193067Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *      # for window() function\n",
    "\n",
    "staticCountsDF = (\n",
    "  staticInputDF\n",
    "    .groupBy(\n",
    "       staticInputDF.action, \n",
    "       window(staticInputDF.time, \"1 hour\"))    \n",
    "    .count()\n",
    ")\n",
    "staticCountsDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsDF.createOrReplaceTempView(\"static_counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:07:08.294420Z",
     "start_time": "2019-01-27T18:07:04.378800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|action|total_count|\n",
      "+------+-----------+\n",
      "|  Open|        184|\n",
      "| Close|         12|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %sql select action, sum(count) as total_count from static_counts group by action\n",
    "df=spark.sql(\"select action, sum(count) as total_count from static_counts group by action\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:08:29.551135Z",
     "start_time": "2019-01-27T18:08:28.600161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+\n",
      "|action|        time|count|\n",
      "+------+------------+-----+\n",
      "| Close|Jul-26 11:00|   11|\n",
      "|  Open|Jul-26 11:00|  179|\n",
      "| Close|Jul-26 12:00|    1|\n",
      "|  Open|Jul-26 12:00|    5|\n",
      "+------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How about a timeline of windowed counts?\n",
    "str='''select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts order by time, action'''\n",
    "df=spark.sql(str)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing\n",
    "Now that we have analyzed the data interactively, let's convert this to a streaming query that continuously updates as data comes. Since we just have a static set of files, we are going to emulate a stream from them by reading one file at a time, in the chronological order they were created. The query we have to write is pretty much the same as the interactive query above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:11:28.376002Z",
     "start_time": "2019-01-27T18:11:28.230085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(jsonSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.action, \n",
    "      window(streamingInputDF.time, \"1 hour\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, streamingCountsDF is a streaming Dataframe (streamingCountsDF.isStreaming was true). You can start streaming computation, by defining the sink and starting it. In our case, we want to interactively query the counts (same queries as above), so we will set the complete set of 1 hour counts to be in a in-memory table (note that this for testing purpose only in Spark 2.0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:12:41.357815Z",
     "start_time": "2019-01-27T18:12:41.106011Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query is a handle to the streaming query that is running in the background. This query is continuously picking up files and updating the windowed counts.\n",
    "\n",
    "Note the status of query in the above cell. The progress bar shows that the query is active. Furthermore, if you expand the > counts above, you will find the number of files they have already processed.\n",
    "\n",
    "Let's wait a bit for a few files to be processed and then interactively query the in-memory counts table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:13:14.124273Z",
     "start_time": "2019-01-27T18:13:09.117005Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(5)  # wait a bit for computation to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:14:17.726133Z",
     "start_time": "2019-01-27T18:14:17.557600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+\n",
      "|action|        time|count|\n",
      "+------+------------+-----+\n",
      "| Close|Jul-26 11:00|   11|\n",
      "|  Open|Jul-26 11:00|  179|\n",
      "| Close|Jul-26 12:00|   59|\n",
      "|  Open|Jul-26 12:00|  188|\n",
      "+------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql('select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:14:38.772422Z",
     "start_time": "2019-01-27T18:14:33.679569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+\n",
      "|action|        time|count|\n",
      "+------+------------+-----+\n",
      "| Close|Jul-26 11:00|   11|\n",
      "|  Open|Jul-26 11:00|  179|\n",
      "| Close|Jul-26 12:00|   59|\n",
      "|  Open|Jul-26 12:00|  188|\n",
      "+------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(5)\n",
    "\n",
    "df=spark.sql('select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:15:51.193627Z",
     "start_time": "2019-01-27T18:15:50.864847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|action|total_count|\n",
      "+------+-----------+\n",
      "| Close|         70|\n",
      "|  Open|        367|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlstr='''select action, sum(count) as total_count from counts group by action order by action'''\n",
    "df=spark.sql(sqlstr)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you keep running the above query repeatedly, you will always find that the number of \"opens\" is more than the number of \"closes\", as expected in a data stream where a \"close\" always appear after corresponding \"open\". This shows that Structured Streaming ensures prefix integrity. Read the blog posts linked below if you want to know more.\n",
    "\n",
    "Note that there are only a few files, so consuming all of them there will be no updates to the counts. Rerun the query if you want to interact with the streaming query again.\n",
    "\n",
    "Finally, you can stop the query running in the background, either by clicking on the 'Cancel' link in the cell of the query, or by executing query.stop(). Either way, when the query is stopped, the status of the corresponding cell above will automatically update to TERMINATED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T18:16:33.442866Z",
     "start_time": "2019-01-27T18:16:33.405308Z"
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
