{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T18:06:20.770661Z",
     "start_time": "2019-01-26T18:06:15.666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6933c6de\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6933c6de"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Random\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .appName(\"GroupBy Test\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T18:09:42.592259Z",
     "start_time": "2019-01-26T18:09:34.428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "args = Array(2, 1000)\n",
       "numMappers = 2\n",
       "numKVPairs = 1000\n",
       "valSize = 1000\n",
       "numReducers = 2\n",
       "pairs1 = MapPartitionsRDD[1] at flatMap at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1] at flatMap at <console>:36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var args=Array(\"2\", \"1000\")\n",
    "\n",
    "val numMappers = if (args.length > 0) args(0).toInt else 2\n",
    "val numKVPairs = if (args.length > 1) args(1).toInt else 1000\n",
    "val valSize = if (args.length > 2) args(2).toInt else 1000\n",
    "val numReducers = if (args.length > 3) args(3).toInt else numMappers\n",
    "\n",
    "val pairs1 = spark.sparkContext.parallelize(0 until numMappers, numMappers).flatMap { p =>\n",
    "  val ranGen = new Random\n",
    "  val arr1 = new Array[(Int, Array[Byte])](numKVPairs)\n",
    "  for (i <- 0 until numKVPairs) {\n",
    "    val byteArr = new Array[Byte](valSize)\n",
    "    ranGen.nextBytes(byteArr)\n",
    "    arr1(i) = (ranGen.nextInt(Int.MaxValue), byteArr)\n",
    "  }\n",
    "  arr1\n",
    "}.cache()\n",
    "// Enforce that everything has been calculated and in cache\n",
    "pairs1.count()\n",
    "\n",
    "println(pairs1.groupByKey(numReducers).count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
