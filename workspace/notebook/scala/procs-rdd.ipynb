{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:12:36.360173Z",
     "start_time": "2019-01-27T14:12:31.342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@7ab6f54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7ab6f54"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"SummarizerExample\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⊕ [spark 统计每天新增用户数 - 董可伦 - CSDN博客](https://blog.csdn.net/dkl12/article/details/80256688)\n",
    "- 对原始数据进行倒排索引\n",
    "- 这样我们只看列一，统计每个日期在列一出现的次数，即为对应日期新增用户数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:13:03.150046Z",
     "start_time": "2019-01-27T14:12:56.085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017-01-01,3)\n",
      "(2017-01-02,1)\n",
      "(2017-01-03,2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1 = ParallelCollectionRDD[0] at parallelize at <console>:31\n",
       "rdd2 = MapPartitionsRDD[1] at map at <console>:37\n",
       "rdd3 = ShuffledRDD[2] at groupByKey at <console>:39\n",
       "rdd4 = MapPartitionsRDD[3] at map at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at map at <console>:41"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = spark.sparkContext.parallelize(\n",
    "  Array(\n",
    "    (\"2017-01-01\", \"a\"), (\"2017-01-01\", \"b\"), (\"2017-01-01\", \"c\"),\n",
    "    (\"2017-01-02\", \"a\"), (\"2017-01-02\", \"b\"), (\"2017-01-02\", \"d\"),\n",
    "    (\"2017-01-03\", \"b\"), (\"2017-01-03\", \"e\"), (\"2017-01-03\", \"f\")))\n",
    "//倒排\n",
    "val rdd2 = rdd1.map(kv => (kv._2, kv._1))\n",
    "//倒排后的key分组\n",
    "val rdd3 = rdd2.groupByKey()\n",
    "//取最小时间\n",
    "val rdd4 = rdd3.map(kv => (kv._2.min, 1))\n",
    "rdd4.countByKey().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⊕ [Spark 解决 某商品日交易额统计 - 一路风景 - CSDN博客](https://blog.csdn.net/zrc199021/article/details/73482528)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:17:27.051164Z",
     "start_time": "2019-01-27T14:17:24.169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1 = ../../data/sales.txt MapPartitionsRDD[7] at textFile at <console>:36\n",
       "rdd2 = MapPartitionsRDD[8] at filter at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at filter at <console>:39"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//加载数据集\n",
    "val rdd1 = sc.textFile(\"../../data/sales.txt\")\n",
    "\n",
    "//过滤\n",
    "val rdd2 = rdd1.filter (log => log.split(\",\").length == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:08:41.961128Z",
     "start_time": "2019-01-27T15:08:41.343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:55: error: overloaded method value agg with alternatives:\n",
       "  (expr: org.apache.spark.sql.Column,exprs: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame <and>\n",
       "  (aggExpr: (String, String),aggExprs: (String, String)*)org.apache.spark.sql.DataFrame\n",
       " cannot be applied to (String, org.apache.spark.sql.Column)\n",
       "       saleDF.groupBy(\"date\").agg(\"date\", sum(\"sale_amount\"))\n",
       "                              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//切分字段\n",
    "//导入 Row\n",
    "import org.apache.spark.sql.Row\n",
    "//取前两个字段\n",
    "val rowRdd3 = rdd2.map { log => Row(log.split(\",\")(0), log.split(\",\")(1).toDouble) }\n",
    "\n",
    "//导入\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//创建映射对应关系\n",
    "val schema = StructType(Array(\n",
    "StructField(\"date\", StringType, true),\n",
    "StructField(\"sale_amount\", DoubleType, true))\n",
    ")\n",
    "\n",
    "// 创建df\n",
    "val saleDF = spark.createDataFrame(rowRdd3, schema)  \n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions.sum\n",
    "\n",
    "saleDF.groupBy(\"date\").agg(\"date\", sum(\"sale_amount\"))\n",
    "    .rdd.map { row => Row(row(1), row(2)) }\n",
    "    .coalesce(1,true)\n",
    "    // .saveAsTextFile(\"/home/shiyanlou/res\")\n",
    "    .foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⊕ [dataframe - Spark SQL: apply aggregate functions to a list of column - Stack Overflow](https://stackoverflow.com/questions/33882894/spark-sql-apply-aggregate-functions-to-a-list-of-column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:27:23.955253Z",
     "start_time": "2019-01-27T14:27:20.451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+---------+\n",
      "|col1|min(col1)|min(col2)|min(col3)|\n",
      "+----+---------+---------+---------+\n",
      "|-1.0|     -1.0|      0.6|      0.2|\n",
      "| 1.0|      1.0|      0.3|      0.0|\n",
      "+----+---------+---------+---------+\n",
      "\n",
      "+----+---------+-----------------+---------+\n",
      "|col1|sum(col1)|        sum(col2)|sum(col3)|\n",
      "+----+---------+-----------------+---------+\n",
      "|-1.0|     -2.0|6.199999999999999|      0.7|\n",
      "| 1.0|      2.0|              0.8|      1.0|\n",
      "+----+---------+-----------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [col1: double, col2: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[col1: double, col2: double ... 1 more field]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sc.parallelize(Seq(\n",
    "  (1.0, 0.3, 1.0), (1.0, 0.5, 0.0),\n",
    "  (-1.0, 0.6, 0.5), (-1.0, 5.6, 0.2))\n",
    ").toDF(\"col1\", \"col2\", \"col3\")\n",
    "\n",
    "df.groupBy($\"col1\").min().show\n",
    "df.groupBy($\"col1\").sum().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:29:29.296655Z",
     "start_time": "2019-01-27T14:29:26.046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+\n",
      "|col1|min(col2)|min(col3)|\n",
      "+----+---------+---------+\n",
      "| tim|      0.6|      0.2|\n",
      "| tom|      0.3|      0.0|\n",
      "+----+---------+---------+\n",
      "\n",
      "+----+-----------------+---------+\n",
      "|col1|        sum(col2)|sum(col3)|\n",
      "+----+-----------------+---------+\n",
      "| tim|6.199999999999999|      0.7|\n",
      "| tom|              0.8|      1.0|\n",
      "+----+-----------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [col1: string, col2: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[col1: string, col2: double ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sc.parallelize(Seq(\n",
    "  (\"tom\", 0.3, 1.0), (\"tom\", 0.5, 0.0),\n",
    "  (\"tim\", 0.6, 0.5), (\"tim\", 5.6, 0.2))\n",
    ").toDF(\"col1\", \"col2\", \"col3\")\n",
    "\n",
    "df.groupBy($\"col1\").min().show\n",
    "df.groupBy($\"col1\").sum().show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
