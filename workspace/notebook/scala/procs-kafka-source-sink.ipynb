{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:53:28.936988Z",
     "start_time": "2019-01-28T11:52:17.048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 for download\n",
      "Obtained 12 files\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.apache.spark spark-sql-kafka-0-10_2.11 2.4.0 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:53:30.804239Z",
     "start_time": "2019-01-28T11:53:24.369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@578b252c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@578b252c"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"Simple Application\").getOrCreate()\n",
    "\n",
    "// For implicit conversions from RDDs to DataFrames\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:53:44.197758Z",
     "start_time": "2019-01-28T11:53:42.536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(time,TimestampType,true), StructField(action,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(time,TimestampType,true), StructField(action,StringType,true))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = new StructType()\n",
    "  .add($\"time\".timestamp)\n",
    "  .add($\"action\".string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:53:54.497911Z",
     "start_time": "2019-01-28T11:53:49.926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IP = localhost\n",
       "TOPIC = events\n",
       "ds1 = [key: binary, value: binary ... 5 more fields]\n",
       "df = [time: timestamp, action: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: timestamp, action: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val IP=\"localhost\"\n",
    "val TOPIC=\"events\"\n",
    "val ds1 = spark\n",
    "          .readStream\n",
    "          .format(\"kafka\")\n",
    "          .option(\"kafka.bootstrap.servers\", IP + \":9092\")\n",
    "          .option(\"zookeeper.connect\", IP + \":2181\")\n",
    "          .option(\"subscribe\", TOPIC)\n",
    "          .option(\"startingOffsets\", \"earliest\")\n",
    "          .option(\"max.poll.records\", 10)\n",
    "          .option(\"failOnDataLoss\", false)\n",
    "          .load()\n",
    "\n",
    "// Now you can use this schema in from_json method like below.\n",
    "\n",
    "val df = ds1.select($\"value\" cast \"string\" as \"json\")\n",
    "            .select(from_json($\"json\", schema) as \"data\")\n",
    "            .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:53:59.476236Z",
     "start_time": "2019-01-28T11:53:57.681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streamingCountsDF = [action: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Same query as staticInputDF\n",
    "val streamingCountsDF = \n",
    "  df\n",
    "    .groupBy($\"action\", window($\"time\", \"1 hour\"))\n",
    "    .count()\n",
    "\n",
    "// Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T10:13:17.937848Z",
     "start_time": "2019-01-28T10:13:16.046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7233e1a1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7233e1a1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")  // keep the size of shuffles small\n",
    "\n",
    "val query =\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        // memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     // counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  // complete = all the counts should be in the table\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T10:13:32.858137Z",
     "start_time": "2019-01-28T10:13:30.194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----+\n",
      "|action|        time|count|\n",
      "+------+------------+-----+\n",
      "| Close|Jul-26 11:00|   11|\n",
      "|  Open|Jul-26 11:00|  179|\n",
      "| Close|Jul-26 12:00|    1|\n",
      "|  Open|Jul-26 12:00|    5|\n",
      "+------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlstr = select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action\n",
       "df = [action: string, time: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[action: string, time: string ... 1 more field]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sqlstr=\"select action, date_format(window.end, \\\"MMM-dd HH:mm\\\") as time, count from counts order by time, action\"\n",
    "var df=spark.sql(sqlstr)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T10:13:58.302970Z",
     "start_time": "2019-01-28T10:13:55.670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|action|total_count|\n",
      "+------+-----------+\n",
      "| Close|         12|\n",
      "|  Open|        184|\n",
      "+------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlstr = select action, sum(count) as total_count from counts group by action order by action\n",
       "df = [action: string, total_count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[action: string, total_count: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sqlstr=\"select action, sum(count) as total_count from counts group by action order by action\"\n",
    "var df=spark.sql(sqlstr)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T11:40:20.781611Z",
     "start_time": "2019-01-28T11:40:20.418Z"
    }
   },
   "outputs": [],
   "source": [
    "// Finally, you can stop the query running in the background, either by clicking on the 'Cancel' link in the cell of the query, or by executing query.stop(). Either way, when the query is stopped, the status of the corresponding cell above will automatically update to TERMINATED.\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sink to kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T12:07:26.004635Z",
     "start_time": "2019-01-28T12:07:25.556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "StructType(StructField(action,StringType,true), StructField(window,StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)),false), StructField(count,LongType,false))\n"
     ]
    }
   ],
   "source": [
    "println(streamingCountsDF.isStreaming)\n",
    "println(streamingCountsDF.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T12:23:29.897684Z",
     "start_time": "2019-01-28T12:23:25.295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkpointLocation = /tmp/temporary-df6e5550-5551-4324-9e18-144f5c613905\n",
       "ds = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@30e22d4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@30e22d4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.UUID\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._\n",
    "\n",
    "val checkpointLocation = \"/tmp/temporary-\" + UUID.randomUUID.toString\n",
    "// Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n",
    "val ds = streamingCountsDF\n",
    "  // .selectExpr(\"CAST(action AS STRING)\", \"CAST(count AS STRING)\")\n",
    "  .select(to_json(struct($\"action\", $\"count\", $\"window.end\")).alias(\"value\"))\n",
    "  .writeStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", IP + \":9092\")\n",
    "  .option(\"topic\", \"events-out\")\n",
    "  .option(\"checkpointLocation\", checkpointLocation)\n",
    "  .outputMode(\"complete\")\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-28T12:44:52.031645Z",
     "start_time": "2019-01-28T12:44:51.606Z"
    }
   },
   "outputs": [],
   "source": [
    "ds.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
