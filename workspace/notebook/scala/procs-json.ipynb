{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:24:02.047307Z",
     "start_time": "2019-01-26T13:23:54.318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@47b4a247\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@47b4a247"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:24:39.611824Z",
     "start_time": "2019-01-26T13:24:31.566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prefix = ../../data\n",
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prefix=\"../../data\"\n",
    "val df = spark.read.json(prefix+\"/resources/people.json\")\n",
    "\n",
    "// Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:25:00.037595Z",
     "start_time": "2019-01-26T13:24:58.152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// This import is needed to use the $-notation\n",
    "import spark.implicits._\n",
    "// Print the schema in a tree format\n",
    "df.printSchema()\n",
    "// root\n",
    "// |-- age: long (nullable = true)\n",
    "// |-- name: string (nullable = true)\n",
    "\n",
    "// Select only the \"name\" column\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:25:16.456029Z",
     "start_time": "2019-01-26T13:25:12.673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Select everybody, but increment the age by 1\n",
    "df.select($\"name\", $\"age\" + 1).show()\n",
    "// +-------+---------+\n",
    "// |   name|(age + 1)|\n",
    "// +-------+---------+\n",
    "// |Michael|     null|\n",
    "// |   Andy|       31|\n",
    "// | Justin|       20|\n",
    "// +-------+---------+\n",
    "\n",
    "// Select people older than 21\n",
    "df.filter($\"age\" > 21).show()\n",
    "// +---+----+\n",
    "// |age|name|\n",
    "// +---+----+\n",
    "// | 30|Andy|\n",
    "// +---+----+\n",
    "\n",
    "// Count people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:25:29.263452Z",
     "start_time": "2019-01-26T13:25:27.444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlDF = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "val sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:26:08.521321Z",
     "start_time": "2019-01-26T13:26:07.810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "// Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "// +----+-------+\n",
    "// | age|   name|\n",
    "// +----+-------+\n",
    "// |null|Michael|\n",
    "// |  30|   Andy|\n",
    "// |  19| Justin|\n",
    "// +----+-------+\n",
    "\n",
    "// Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets\n",
    "Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:28:04.944941Z",
     "start_time": "2019-01-26T13:28:03.839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T13:28:12.819627Z",
     "start_time": "2019-01-26T13:28:07.887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 32|\n",
      "+----+---+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "caseClassDS = [name: string, age: bigint]\n",
       "primitiveDS = [value: int]\n",
       "path = ../../data/resources/people.json\n",
       "peopleDS = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Encoders are created for case classes\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()\n",
    "// +----+---+\n",
    "// |name|age|\n",
    "// +----+---+\n",
    "// |Andy| 32|\n",
    "// +----+---+\n",
    "\n",
    "// Encoders for most common types are automatically provided by importing spark.implicits._\n",
    "val primitiveDS = Seq(1, 2, 3).toDS()\n",
    "primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val path = prefix+\"/resources/people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperating with RDDs\n",
    "[loc](https://spark.apache.org/docs/latest/sql-getting-started.html#interoperating-with-rdds)\n",
    "\n",
    "Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection-based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    "\n",
    "The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T16:28:50.008948Z",
     "start_time": "2019-01-26T16:28:45.189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|Name: Justin|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "peopleDF = [name: string, age: bigint]\n",
       "teenagersDF = [name: string, age: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, age: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "import spark.implicits._\n",
    "\n",
    "// Create an RDD of Person objects from a text file, convert it to a Dataframe\n",
    "val peopleDF = spark.sparkContext\n",
    "  .textFile(prefix+\"/resources/people.txt\")\n",
    "  .map(_.split(\",\"))\n",
    "  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))\n",
    "  .toDF()\n",
    "// Register the DataFrame as a temporary view\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by Spark\n",
    "val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "\n",
    "// The columns of a row in the result can be accessed by field index\n",
    "teenagersDF.map(teenager => \"Name: \" + teenager(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T16:29:26.747152Z",
     "start_time": "2019-01-26T16:29:22.350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|Name: Justin|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mapEncoder = class[value[0]: binary]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Map(name -> Justin, age -> 19))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// or by field name\n",
    "teenagersDF.map(teenager => \"Name: \" + teenager.getAs[String](\"name\")).show()\n",
    "// +------------+\n",
    "// |       value|\n",
    "// +------------+\n",
    "// |Name: Justin|\n",
    "// +------------+\n",
    "\n",
    "// No pre-defined encoders for Dataset[Map[K,V]], define explicitly\n",
    "implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]\n",
    "// Primitive types and case classes can be also defined as\n",
    "// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()\n",
    "\n",
    "// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]\n",
    "teenagersDF.map(teenager => teenager.getValuesMap[Any](List(\"name\", \"age\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T14:12:35.094169Z",
     "start_time": "2019-01-26T14:12:34.359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiaofeiwu/jcloud/assets/langs/workspace/spark/spark-2.4.0-bin-hadoop2.7"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkHome = /Users/xiaofeiwu/jcloud/assets/langs/workspace/spark/spark-2.4.0-bin-hadoop2.7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/Users/xiaofeiwu/jcloud/assets/langs/workspace/spark/spark-2.4.0-bin-hadoop2.7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkHome=sys.env(\"SPARK_HOME\")\n",
    "print(s\"${sparkHome}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T14:12:45.245381Z",
     "start_time": "2019-01-26T14:12:42.279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 62, Lines with b: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkHome = /Users/xiaofeiwu/jcloud/assets/langs/workspace/spark/spark-2.4.0-bin-hadoop2.7\n",
       "logFile = /Users/xiaofeiwu/jcloud/assets/langs/workspace/spark/spark-2.4.0-bin-hadoop2.7/README.md\n",
       "spark = org.apache.spark.sql.SparkSession@47b4a247\n",
       "logData = [value: string]\n",
       "numAs = 62\n",
       "numBs = 31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkHome=sys.env(\"SPARK_HOME\")\n",
    "val logFile = s\"${sparkHome}/README.md\" // Should be some file on your system\n",
    "val spark = SparkSession.builder.appName(\"Simple Application\").getOrCreate()\n",
    "val logData = spark.read.textFile(logFile).cache()\n",
    "val numAs = logData.filter(line => line.contains(\"a\")).count()\n",
    "val numBs = logData.filter(line => line.contains(\"b\")).count()\n",
    "println(s\"Lines with a: $numAs, Lines with b: $numBs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
