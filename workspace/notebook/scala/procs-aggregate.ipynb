{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:38:01.772117Z",
     "start_time": "2019-01-27T14:37:56.369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@1ad41540\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@1ad41540"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"SummarizerExample\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T14:38:23.983100Z",
     "start_time": "2019-01-27T14:38:12.204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+\n",
      "|col1|min(col2)|min(col3)|\n",
      "+----+---------+---------+\n",
      "| tim|      0.6|      0.2|\n",
      "| tom|      0.3|      0.0|\n",
      "+----+---------+---------+\n",
      "\n",
      "+----+-----------------+---------+\n",
      "|col1|        sum(col2)|sum(col3)|\n",
      "+----+-----------------+---------+\n",
      "| tim|6.199999999999999|      0.7|\n",
      "| tom|              0.8|      1.0|\n",
      "+----+-----------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [col1: string, col2: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[col1: string, col2: double ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sc.parallelize(Seq(\n",
    "  (\"tom\", 0.3, 1.0), (\"tom\", 0.5, 0.0),\n",
    "  (\"tim\", 0.6, 0.5), (\"tim\", 5.6, 0.2))\n",
    ").toDF(\"col1\", \"col2\", \"col3\")\n",
    "\n",
    "df.groupBy($\"col1\").min().show\n",
    "df.groupBy($\"col1\").sum().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:00:32.405674Z",
     "start_time": "2019-01-27T15:00:31.222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+---------+\n",
      "|col1|        sum(col2)|sum(col3)|\n",
      "+----+-----------------+---------+\n",
      "| tim|6.199999999999999|      0.7|\n",
      "| tom|              0.8|      1.0|\n",
      "+----+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"col1\").sum(\"col2\", \"col3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:05:43.879490Z",
     "start_time": "2019-01-27T15:05:41.098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------------+---------+\n",
      "|col1|avg(col1)|         avg(col2)|avg(col3)|\n",
      "+----+---------+------------------+---------+\n",
      "| tim|     null|3.0999999999999996|     0.35|\n",
      "| tom|     null|               0.4|      0.5|\n",
      "+----+---------+------------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "exprs = Map(col1 -> mean, col2 -> mean, col3 -> mean)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(col1 -> mean, col2 -> mean, col3 -> mean)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exprs = df.columns.map((_ -> \"mean\")).toMap\n",
    "df.groupBy($\"col1\").agg(exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:06:43.438968Z",
     "start_time": "2019-01-27T15:06:41.312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------------+---------+\n",
      "|col1|sum(col1)|        sum(col2)|sum(col3)|\n",
      "+----+---------+-----------------+---------+\n",
      "| tim|     null|6.199999999999999|      0.7|\n",
      "| tom|     null|              0.8|      1.0|\n",
      "+----+---------+-----------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "exprs = Array(sum(col1), sum(col2), sum(col3))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(sum(col1), sum(col2), sum(col3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.sum\n",
    "\n",
    "val exprs = df.columns.map(sum(_))\n",
    "df.groupBy($\"col1\").agg(exprs.head, exprs.tail: _*).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:17:16.060667Z",
     "start_time": "2019-01-27T15:17:14.743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+--------+--------+\n",
      "|col1|         col2_sum|col3_sum|col3_avg|\n",
      "+----+-----------------+--------+--------+\n",
      "| tim|6.199999999999999|     0.7|    0.35|\n",
      "| tom|              0.8|     1.0|     0.5|\n",
      "+----+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "df.groupBy($\"col1\").agg(\n",
    "    sum(\"col2\").alias(\"col2_sum\"), \n",
    "    sum(\"col3\").alias(\"col3_sum\"), \n",
    "    avg(\"col3\").alias(\"col3_avg\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:14:07.959321Z",
     "start_time": "2019-01-27T15:14:07.637Z"
    }
   },
   "source": [
    "See the example below - Using Maps\n",
    "\n",
    "```scala\n",
    "val Claim1 = StructType(Seq(StructField(\"pid\", StringType, true),StructField(\"diag1\", StringType, true),StructField(\"diag2\", StringType, true), StructField(\"allowed\", IntegerType, true), StructField(\"allowed1\", IntegerType, true)))\n",
    "val claimsData1 = Seq((\"PID1\", \"diag1\", \"diag2\", 100, 200), (\"PID1\", \"diag2\", \"diag3\", 300, 600), (\"PID1\", \"diag1\", \"diag5\", 340, 680), (\"PID2\", \"diag3\", \"diag4\", 245, 490), (\"PID2\", \"diag2\", \"diag1\", 124, 248))\n",
    "\n",
    "val claimRDD1 = sc.parallelize(claimsData1)\n",
    "val claimRDDRow1 = claimRDD1.map(p => Row(p._1, p._2, p._3, p._4, p._5))\n",
    "val claimRDD2DF1 = sqlContext.createDataFrame(claimRDDRow1, Claim1)\n",
    "\n",
    "val l = List(\"allowed\", \"allowed1\")\n",
    "val exprs = l.map((_ -> \"sum\")).toMap\n",
    "claimRDD2DF1.groupBy(\"pid\").agg(exprs) show false\n",
    "val exprs = Map(\"allowed\" -> \"sum\", \"allowed1\" -> \"avg\")\n",
    "\n",
    "claimRDD2DF1.groupBy(\"pid\").agg(exprs) show false\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
