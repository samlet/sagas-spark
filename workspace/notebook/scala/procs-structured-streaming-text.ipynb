{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:34:33.978432Z",
     "start_time": "2019-01-27T17:34:28.335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@29c7dbbb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@29c7dbbb"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"StructuredNetworkWordCount\")\n",
    "  .config(\"spark.master\", \"local\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:36:02.807918Z",
     "start_time": "2019-01-27T17:34:37.191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|               value|count|\n",
      "+--------------------+-----+\n",
      "|              online|    1|\n",
      "|              graphs|    1|\n",
      "|          [\"Building|    1|\n",
      "|          [\"Parallel|    1|\n",
      "|              thread|    1|\n",
      "|       documentation|    3|\n",
      "|            command,|    2|\n",
      "|         abbreviated|    1|\n",
      "|            overview|    1|\n",
      "|                rich|    1|\n",
      "|                 set|    2|\n",
      "|         -DskipTests|    1|\n",
      "|                name|    1|\n",
      "|page](http://spar...|    1|\n",
      "|        [\"Specifying|    1|\n",
      "|              stream|    1|\n",
      "|                 not|    1|\n",
      "|                run:|    1|\n",
      "|            programs|    2|\n",
      "|     ./dev/run-tests|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.InterruptedException\n",
       "Message: null\n",
       "StackTrace:   at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n",
       "  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n",
       "  at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)\n",
       "  at org.apache.spark.sql.execution.streaming.StreamExecution.awaitTermination(StreamExecution.scala:467)\n",
       "  at org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.awaitTermination(StreamingQueryWrapper.scala:53)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Create DataFrame representing the stream of input lines from connection to host:port\n",
    "// val lines = spark.readStream\n",
    "//   .format(\"socket\")\n",
    "//   .option(\"host\", host)\n",
    "//   .option(\"port\", port)\n",
    "//   .load()\n",
    "val path = \"./text\"\n",
    "val lines = spark.readStream\n",
    "  .format(\"text\")\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .option(\"fileNameOnly\", true)\n",
    "  .load(path)\n",
    "\n",
    "// Split the lines into words\n",
    "val words = lines.as[String].flatMap(_.split(\" \"))\n",
    "\n",
    "// Generate running word count\n",
    "val wordCounts = words.groupBy(\"value\").count()\n",
    "\n",
    "// Start running the query that prints the running counts to the console\n",
    "val query = wordCounts.writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T15:59:44.879859Z",
     "start_time": "2019-01-27T15:59:44.103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.readStream\n",
    "  .format(\"text\")\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .load(\"words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⊕ [spark-structured-streaming-book/spark-sql-streaming-MemoryStream.adoc at master · jaceklaskowski/spark-structured-streaming-book](https://github.com/jaceklaskowski/spark-structured-streaming-book/blob/master/spark-sql-streaming-MemoryStream.adoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:38:28.312478Z",
     "start_time": "2019-01-27T17:38:23.726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ctx = org.apache.spark.sql.SQLContext@1ecf86ae\n",
       "intsIn = MemoryStream[value#33]\n",
       "ints = [window: struct<start: timestamp, end: timestamp>, total: bigint]\n",
       "totalsOver5mins = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@30d392c1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@30d392c1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val ctx = spark.sqlContext\n",
    "\n",
    "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// It uses two implicits: Encoder[Int] and SQLContext\n",
    "val intsIn = MemoryStream[Int]\n",
    "\n",
    "val ints = intsIn.toDF\n",
    "  .withColumn(\"t\", current_timestamp())\n",
    "  .withWatermark(\"t\", \"5 minutes\")\n",
    "  .groupBy(window($\"t\", \"5 minutes\") as \"window\")\n",
    "  .agg(count(\"*\") as \"total\")\n",
    "\n",
    "import org.apache.spark.sql.streaming.{OutputMode, Trigger}\n",
    "import scala.concurrent.duration._\n",
    "val totalsOver5mins = ints.\n",
    "  writeStream.\n",
    "  format(\"memory\").\n",
    "  queryName(\"totalsOver5mins\").\n",
    "  outputMode(OutputMode.Append).\n",
    "  trigger(Trigger.ProcessingTime(10.seconds)).\n",
    "  start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:38:42.572855Z",
     "start_time": "2019-01-27T17:38:41.729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zeroOffset = 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zeroOffset = intsIn.addData(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:39:21.929211Z",
     "start_time": "2019-01-27T17:38:53.209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|window|total|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalsOver5mins.processAllAvailable()\n",
    "spark.table(\"totalsOver5mins\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T17:41:43.217594Z",
     "start_time": "2019-01-27T17:41:42.633Z"
    }
   },
   "outputs": [],
   "source": [
    "// intsOut.show\n",
    "totalsOver5mins.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
