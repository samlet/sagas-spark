{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T16:49:25.036362Z",
     "start_time": "2019-01-26T16:49:18.972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@1d0425dc\n",
       "prefix = ../../data\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "../../data"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"Simple Application\").getOrCreate()\n",
    "// For implicit conversions from RDDs to DataFrames\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val prefix=\"../../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "The built-in DataFrames functions provide common aggregations such as count(), countDistinct(), avg(), max(), min(), etc. While those functions are designed for DataFrames, Spark SQL also has type-safe versions for some of them in Scala and Java to work with strongly typed Datasets. Moreover, users are not limited to the predefined aggregate functions and can create their own.\n",
    "\n",
    "## Untyped User-Defined Aggregate Functions\n",
    "Users have to extend the UserDefinedAggregateFunction abstract class to implement a custom untyped aggregate function. For example, a user-defined average can look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T16:50:08.918858Z",
     "start_time": "2019-01-26T16:50:00.484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n",
      "+--------------+\n",
      "|average_salary|\n",
      "+--------------+\n",
      "|        3750.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object MyAverage\n",
       "df = [name: string, salary: bigint]\n",
       "result = [average_salary: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[average_salary: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "object MyAverage extends UserDefinedAggregateFunction {\n",
    "  // Data types of input arguments of this aggregate function\n",
    "  def inputSchema: StructType = StructType(StructField(\"inputColumn\", LongType) :: Nil)\n",
    "  // Data types of values in the aggregation buffer\n",
    "  def bufferSchema: StructType = {\n",
    "    StructType(StructField(\"sum\", LongType) :: StructField(\"count\", LongType) :: Nil)\n",
    "  }\n",
    "  // The data type of the returned value\n",
    "  def dataType: DataType = DoubleType\n",
    "  // Whether this function always returns the same output on the identical input\n",
    "  def deterministic: Boolean = true\n",
    "  // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to\n",
    "  // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides\n",
    "  // the opportunity to update its values. Note that arrays and maps inside the buffer are still\n",
    "  // immutable.\n",
    "  def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "    buffer(0) = 0L\n",
    "    buffer(1) = 0L\n",
    "  }\n",
    "  // Updates the given aggregation buffer `buffer` with new input data from `input`\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "    if (!input.isNullAt(0)) {\n",
    "      buffer(0) = buffer.getLong(0) + input.getLong(0)\n",
    "      buffer(1) = buffer.getLong(1) + 1\n",
    "    }\n",
    "  }\n",
    "  // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)\n",
    "    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)\n",
    "  }\n",
    "  // Calculates the final result\n",
    "  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)\n",
    "}\n",
    "\n",
    "// Register the function to access it\n",
    "spark.udf.register(\"myAverage\", MyAverage)\n",
    "\n",
    "val df = spark.read.json(prefix+\"/resources/employees.json\")\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "df.show()\n",
    "// +-------+------+\n",
    "// |   name|salary|\n",
    "// +-------+------+\n",
    "// |Michael|  3000|\n",
    "// |   Andy|  4500|\n",
    "// | Justin|  3500|\n",
    "// |  Berta|  4000|\n",
    "// +-------+------+\n",
    "\n",
    "val result = spark.sql(\"SELECT myAverage(salary) as average_salary FROM employees\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-Safe User-Defined Aggregate Functions\n",
    "User-defined aggregations for strongly typed Datasets revolve around the Aggregator abstract class. For example, a type-safe user-defined average can look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T16:53:04.537113Z",
     "start_time": "2019-01-26T16:53:03.553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Employee\n",
       "defined class Average\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Employee(name: String, salary: Long)\n",
    "case class Average(var sum: Long, var count: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-26T16:53:11.762397Z",
     "start_time": "2019-01-26T16:53:07.906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n",
      "+--------------+\n",
      "|average_salary|\n",
      "+--------------+\n",
      "|        3750.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object MyAverage\n",
       "ds = [name: string, salary: bigint]\n",
       "averageSalary = myaverage() AS `average_salary`\n",
       "result = [average_salary: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[average_salary: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Encoder, Encoders, SparkSession}\n",
    "import org.apache.spark.sql.expressions.Aggregator\n",
    "\n",
    "object MyAverage extends Aggregator[Employee, Average, Double] {\n",
    "  // A zero value for this aggregation. Should satisfy the property that any b + zero = b\n",
    "  def zero: Average = Average(0L, 0L)\n",
    "  // Combine two values to produce a new value. For performance, the function may modify `buffer`\n",
    "  // and return it instead of constructing a new object\n",
    "  def reduce(buffer: Average, employee: Employee): Average = {\n",
    "    buffer.sum += employee.salary\n",
    "    buffer.count += 1\n",
    "    buffer\n",
    "  }\n",
    "  // Merge two intermediate values\n",
    "  def merge(b1: Average, b2: Average): Average = {\n",
    "    b1.sum += b2.sum\n",
    "    b1.count += b2.count\n",
    "    b1\n",
    "  }\n",
    "  // Transform the output of the reduction\n",
    "  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count\n",
    "  // Specifies the Encoder for the intermediate value type\n",
    "  def bufferEncoder: Encoder[Average] = Encoders.product\n",
    "  // Specifies the Encoder for the final output value type\n",
    "  def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n",
    "}\n",
    "\n",
    "val ds = spark.read.json(prefix+\"/resources/employees.json\").as[Employee]\n",
    "ds.show()\n",
    "// +-------+------+\n",
    "// |   name|salary|\n",
    "// +-------+------+\n",
    "// |Michael|  3000|\n",
    "// |   Andy|  4500|\n",
    "// | Justin|  3500|\n",
    "// |  Berta|  4000|\n",
    "// +-------+------+\n",
    "\n",
    "// Convert the function to a `TypedColumn` and give it a name\n",
    "val averageSalary = MyAverage.toColumn.name(\"average_salary\")\n",
    "val result = ds.select(averageSalary)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
